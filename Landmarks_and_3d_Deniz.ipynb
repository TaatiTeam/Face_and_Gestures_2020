{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to use\n",
    "import pyrealsense2 as rs\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy import signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNN models for face detection and landmark localization | send the models to CUDA is available | load model weights || this procedure only happens once \n",
    "from face_alignment.utils import *\n",
    "from face_alignment import api as face_alignment\n",
    "from face_alignment.models import FAN\n",
    "from face_alignment.detection.sfd import sfd_detector\n",
    "\n",
    "def load_weights(model, filename):\n",
    "    sd = torch.load(filename, map_location=lambda storage, loc: storage)\n",
    "    names = set(model.state_dict().keys())\n",
    "    for n in list(sd.keys()): \n",
    "        if n not in names and n+'_raw' in names:\n",
    "            if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]\n",
    "            del sd[n]\n",
    "    model.load_state_dict(sd)\n",
    "\n",
    "face_alignment_model = r\"./models/2DFAN4-11f355bf06.pth.tar\"\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#Face alignement\n",
    "network_size = 4\n",
    "face_alignment_net = FAN(network_size)\n",
    "load_weights(face_alignment_net,face_alignment_model)\n",
    "face_alignment_net.to(device)\n",
    "face_alignment_net.eval()\n",
    "#face detection \n",
    "face_detector_model = r\"./models/s3fd-619a316812.pth\"\n",
    "face_detection_net = sfd_detector.SFDDetector(device=device, path_to_detector=face_detector_model, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_from_bag(BAG_File):\n",
    "\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "\n",
    "    rs.config.enable_device_from_file(config, BAG_File, repeat_playback=False)\n",
    "    print('here1')\n",
    "    config.enable_all_streams()\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # create alignment object\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    # inform the device that this is not live streaming from camera\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(False)\n",
    "    duration = playback.get_duration()\n",
    "\n",
    "    true_frame_number = []\n",
    "    frame_number = []\n",
    "    time_st = []\n",
    "\n",
    "    num_frame = 0\n",
    "\n",
    "\n",
    "    Color_Frames = []#{}\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames = pipeline.wait_for_frames(100)  #get frame from file \n",
    "\n",
    "            this_frame = frames.get_frame_number()  #get frame number \n",
    "\n",
    "            if (num_frame != 0) and (true_frame_number[-1] == this_frame): #verify that frame number is not repeated \n",
    "                #if frame number is repeated then replace the stored information \n",
    "                aligned_frames = align.process(frames)\n",
    "\n",
    "                #take color and depth from frame, if any to these is not available then skip the frame\n",
    "                aligned_depth = aligned_frames.get_depth_frame()\n",
    "                aligned_color = aligned_frames.get_color_frame()\n",
    "\n",
    "                # validate that both frames are available\n",
    "                if not aligned_depth or not aligned_color:\n",
    "                    continue\n",
    "\n",
    "                time_stamp = frames.get_timestamp()\n",
    "                true_frame_number[-1] = frames.get_frame_number()\n",
    "                time_st[-1] = time_stamp \n",
    "\n",
    "                # transform to np array\n",
    "\n",
    "                color_data = np.asanyarray(aligned_color.as_frame().get_data(), dtype=np.int)\n",
    "                #depth_data = np.asanyarray(aligned_depth.as_frame().get_data(), dtype=np.int)\n",
    "                # adjust depth data in meters\n",
    "                #depth_data *= depth_scale\n",
    "\n",
    "                Color_Frames[-1] = color_data\n",
    "\n",
    "            else:\n",
    "                #if frame number is not repeated then append the stored information \n",
    "                aligned_frames = align.process(frames)\n",
    "\n",
    "                #take color and depth from frame, if any to these is not available then skip the frame\n",
    "                aligned_depth = aligned_frames.get_depth_frame()\n",
    "                aligned_color = aligned_frames.get_color_frame()\n",
    "\n",
    "                # validate that both frames are available\n",
    "                if not aligned_depth or not aligned_color:\n",
    "                    continue\n",
    "\n",
    "                time_stamp = frames.get_timestamp()\n",
    "                true_frame_number.append(frames.get_frame_number())\n",
    "                time_st.append(time_stamp )\n",
    "\n",
    "                # transform to np array\n",
    "\n",
    "                color_data = np.asanyarray(aligned_color.as_frame().get_data(), dtype=np.int)\n",
    "                #depth_data = np.asanyarray(aligned_depth.as_frame().get_data(), dtype=np.int)\n",
    "                # adjust depth data in meters\n",
    "                #depth_data *= depth_scale\n",
    "\n",
    "                Color_Frames.append(color_data)\n",
    "                #Depth_Frames.append(depth_data\n",
    "\n",
    "                frame_number.append(num_frame)\n",
    "                num_frame += 1\n",
    "    \n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "    print('here2')\n",
    "\n",
    "    duration_movie = duration.total_seconds()\n",
    "    FPS = num_frame/duration_movie\n",
    "    height, width,_ =  Color_Frames[0].shape\n",
    "    print(FPS)\n",
    "    color_file = BAG_File[:-4]+'_color.mp4'\n",
    "    print(color_file)\n",
    "    video = VideoWriter(color_file, 0, int(FPS), (width,height))\n",
    "\n",
    "    for k in range(num_frame):\n",
    "        frame_to_save = Color_Frames[k].astype('uint8')\n",
    "        video.write(frame_to_save)\n",
    "\n",
    "    video.release()    \n",
    "\n",
    "    print('here4')\n",
    "    cvs_frame_info = BAG_File[:-4]+'_frameInfoColor.csv'\n",
    "    df_cols = ['Actual_Frame_Number', 'Frame_Time_Stamp', 'Frame_Number_in_Video']\n",
    "    df = pd.DataFrame(columns=df_cols)\n",
    "    df['Actual_Frame_Number'] = true_frame_number\n",
    "    df['Frame_Time_Stamp'] = (np.array(time_st)-time_st[0])/1000\n",
    "    df['Frame_Number_in_Video'] = frame_number\n",
    "\n",
    "    df.to_csv(cvs_frame_info)\n",
    "    print(cvs_frame_info)\n",
    "    \n",
    "\n",
    "def get_landmarks(BAG_File, face_detection_net, face_alignment_net):\n",
    "    #load information from the video \n",
    "    video_frame_info = pd.read_csv(BAG_File[:-4]+'_frameInfoColor.csv')\n",
    "    true_frame_number = video_frame_info['Actual_Frame_Number'].values\n",
    "    time_st = video_frame_info['Frame_Time_Stamp'].values\n",
    "    \n",
    "    \n",
    "    localize_face = 0\n",
    "    # localize_face = 0 -> Face is localized at a single frame in the video (the middle frame)\n",
    "    # localize_face = -1 -> Face is localized at each frame of the video\n",
    "    # localize_face = n -> face is localized every n frames \n",
    "\n",
    "    # we will start by localizing the face in the middel of the video, if additional information is needed \n",
    "    # then will be added as required\n",
    "    color_file = BAG_File[:-4]+'_color.mp4'\n",
    "    video_handler = cv2.VideoCapture(color_file)  # read the video\n",
    "    num_frames = int(video_handler.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_fps = int(video_handler.get(cv2.CAP_PROP_FPS))\n",
    "    video_handler.set(cv2.CAP_PROP_POS_FRAMES, num_frames//2)\n",
    "\n",
    "    success, image = video_handler.read()\n",
    "\n",
    "    if success: \n",
    "        detected_faces = face_detection_net.detect_from_image(image)\n",
    "        for i, d in enumerate(detected_faces):\n",
    "            center = torch.FloatTensor(\n",
    "                [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0])\n",
    "            center[1] = center[1] - (d[3] - d[1]) * 0.12\n",
    "            scale = (d[2] - d[0] + d[3] - d[1]) / face_detection_net.reference_scale\n",
    "    video_handler.release()\n",
    "\n",
    "    # at this point we have the position of the face in the mid frame. Let's use that info\n",
    "\n",
    "    #create a dataframe that will store all the information \n",
    "    df_cols = [\"BAG_Frame_number\",\"Video_Frame_number\", \"bbox_top_x\", \"bbox_top_y\", \"bbox_bottom_x\", \"bbox_bottom_y\"]\n",
    "    for i in range(0,68):\n",
    "        num=str(i)\n",
    "        xx = 'landmark_'+num+'_x'\n",
    "        yy = 'landmark_'+num+'_y'\n",
    "        df_cols.append(xx)\n",
    "        df_cols.append(yy)\n",
    "\n",
    "    LandmarkDataFrame = pd.DataFrame(columns = df_cols)\n",
    "\n",
    "    # re-position the video handler at the first frame and start going frame by frame\n",
    "    video_handler = cv2.VideoCapture(color_file)  # read the video\n",
    "    k = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success, image = video_handler.read()\n",
    "        if success:\n",
    "\n",
    "            if localize_face == 0:\n",
    "                #do not localize the face, use previous info \n",
    "                pass \n",
    "            elif localize_face == -1 :\n",
    "                #localize the face at each frame, upd\n",
    "                update_detected_faces = face_detection_net.detect_from_image(image)\n",
    "                for i, d in enumerate(update_detected_face):\n",
    "\n",
    "                    if d[4]>=0.8:\n",
    "                        #do we trust the face localizer, if yes (>0.8) then update the bounding box, \n",
    "                        # if not (<0.8) don't update the bounding box\n",
    "                        detected_faces = update_detected_face\n",
    "                        center = torch.FloatTensor(\n",
    "                            [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0])\n",
    "                        center[1] = center[1] - (d[3] - d[1]) * 0.12\n",
    "                        scale = (d[2] - d[0] + d[3] - d[1]) / face_detection_net.reference_scale\n",
    "\n",
    "            else:\n",
    "                #only update every n frames\n",
    "                if (k+1)%localize_face == 0:\n",
    "                    update_detected_face = face_detection_net.detect_from_image(image)\n",
    "                    for i, d in enumerate(detected_faces):\n",
    "\n",
    "                        if d[4]>=0.8:\n",
    "                            #do we trust the face localizer, if yes (>0.8) then update the bounding box, \n",
    "                            # if not (<0.8) don't update the bounding box\n",
    "                            detected_faces = update_detected_face\n",
    "                            center = torch.FloatTensor(\n",
    "                                [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0])\n",
    "                            center[1] = center[1] - (d[3] - d[1]) * 0.12\n",
    "                            scale = (d[2] - d[0] + d[3] - d[1]) / face_detection_net.reference_scale\n",
    "\n",
    "\n",
    "            inp = crop(image, center, scale)\n",
    "            inp = torch.from_numpy(inp.transpose(\n",
    "                        (2, 0, 1))).float()\n",
    "            inp = inp.to(device)\n",
    "            inp.div_(255).unsqueeze_(0)\n",
    "\n",
    "            out = face_alignment_net(inp)[-1].detach() #[-1] is to get the output of the last hourglass block\n",
    "            out = out.cpu()\n",
    "            pts, pts_img = get_preds_fromhm(out, center, scale)\n",
    "\n",
    "            pts_img = pts_img.view(68, 2)\n",
    "\n",
    "\n",
    "\n",
    "            # Store everything in a dataframe\n",
    "            datus = []\n",
    "            datus.append(true_frame_number[int(k)])  #frame number provided by the .bag file \n",
    "            datus.append(int(k)+1)  # frame number in the color_only video \n",
    "\n",
    "            datus.append(detected_faces[0][0])  #top\n",
    "            datus.append(detected_faces[0][1])  #left\n",
    "            datus.append(detected_faces[0][2])  #bottom\n",
    "            datus.append(detected_faces[0][3])  #right\n",
    "\n",
    "            all_landmarks = pts_img.numpy()\n",
    "            for x,y in all_landmarks:\n",
    "                datus.append(x), datus.append(y)  #x and y position of each landmark\n",
    "\n",
    "            LandmarkDataFrame = LandmarkDataFrame.append(pd.Series(datus,index = df_cols), \n",
    "                                   ignore_index = True)\n",
    "\n",
    "            k +=1 \n",
    "\n",
    "\n",
    "    #add time to landmarks \n",
    "    LandmarkDataFrame.insert(loc=1, column='Time_Stamp (s)', value=time_st)\n",
    "\n",
    "    landmark_file = BAG_File[:-4]+'_landmarks.csv'\n",
    "    print(landmark_file)\n",
    "    LandmarkDataFrame.to_csv(landmark_file) \n",
    "\n",
    "def smooth_landmarks(BAG_File):\n",
    "    #Smooth landmarks positions and generate a new video showing landmarks\n",
    "    LandmarkDataFrame = pd.read_csv(BAG_File[:-4]+'_landmarks.csv', index_col=0)\n",
    "    windowlength=5\n",
    "    for i in range(68):\n",
    "        num=str(i)\n",
    "        xx = LandmarkDataFrame['landmark_'+num+'_x'].values\n",
    "        xx_med = signal.medfilt(xx,kernel_size=windowlength)\n",
    "\n",
    "        yy = LandmarkDataFrame['landmark_'+num+'_y'].values\n",
    "        yy_med = signal.medfilt(yy,kernel_size=windowlength)  \n",
    "\n",
    "        LandmarkDataFrame['landmark_'+num+'_x'] = xx_med\n",
    "        LandmarkDataFrame['landmark_'+num+'_y'] = yy_med\n",
    "\n",
    "    landmark_file = BAG_File[:-4]+'_landmarksFiltered.csv'\n",
    "    LandmarkDataFrame.to_csv(landmark_file)    \n",
    "\n",
    "    color_file = BAG_File[:-4]+'_color.mp4'\n",
    "    video_handler = cv2.VideoCapture(color_file)  # read the video\n",
    "    FPS = video_handler.get(cv2.CAP_PROP_FPS)\n",
    "    width = video_handler.get(3)   # float\n",
    "    height = video_handler.get(4) # float\n",
    "    \n",
    "    color_file_landmark = BAG_File[:-4]+'_colorlandmark.mp4'\n",
    "    video = VideoWriter(color_file_landmark, -1, int(FPS), (int(width),int(height)))\n",
    "\n",
    "    success = True \n",
    "    k=0\n",
    "    for k in range(len(LandmarkDataFrame)):\n",
    "        success, image = video_handler.read()\n",
    "\n",
    "        frame_number=k+1\n",
    "        frame_information = LandmarkDataFrame.loc[LandmarkDataFrame['Video_Frame_number'] == frame_number].values\n",
    "        shape = np.array([frame_information[0][7:]])\n",
    "        shape = np.reshape(shape.astype(np.int), (-1, 2))\n",
    "        bb = np.array([frame_information[0][3:7]])\n",
    "\n",
    "        \n",
    "        for j, (x, y) in enumerate(shape):\n",
    "            if x is np.NaN:\n",
    "                continue\n",
    "            elif j<17:\n",
    "                continue\n",
    "            else:\n",
    "                cv2.rectangle(image, (int(bb[0][0]), int(bb[0][1])), (int(bb[0][2]), int(bb[0][3])), (255,0,0), 1)\n",
    "                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "        frame_to_save = image\n",
    "        video.write(frame_to_save)\n",
    "        k +=1\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    \n",
    "def get_3d_data(BAG_File):\n",
    "    #load landmakrs information\n",
    "    DF_landmarks = pd.read_csv(BAG_File[:-4]+'_landmarksFiltered.csv', index_col=0)\n",
    "\n",
    "    #create dataframe to store information about 3d position of landmarks\n",
    "    df_cols_p1 = [\"BAG_Frame_number\",\"Video_Frame_number\",]\n",
    "    for i in range(0,68):\n",
    "        num=str(i)\n",
    "        xx = 'landmark_'+num\n",
    "        df_cols_p1.append(xx)\n",
    "        df_cols_p1.append(xx)\n",
    "        df_cols_p1.append(xx)\n",
    "\n",
    "    df_cols_p2 = [\"\",\"\"]\n",
    "    for i in range(0,68):\n",
    "        df_cols_p2.append(\"x\")\n",
    "        df_cols_p2.append(\"y\")\n",
    "        df_cols_p2.append(\"z\")\n",
    "\n",
    "    header = [np.array(df_cols_p1), \n",
    "    np.array(df_cols_p2)] \n",
    "\n",
    "    DF_3dpositions= pd.DataFrame(columns = header)\n",
    "\n",
    "    # start the process of extracting the video information for each video\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "\n",
    "    rs.config.enable_device_from_file(config, BAG_File, repeat_playback=False)\n",
    "\n",
    "    config.enable_all_streams()\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # create alignment object\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    # Getting the depth sensor's depth scale (see rs-align example for explanation)\n",
    "    depth_sensor = profile.get_device().first_depth_sensor()\n",
    "    depth_scale = depth_sensor.get_depth_scale()\n",
    "\n",
    "    # inform the device that this is not live streaming from camera\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(False)\n",
    "    duration = playback.get_duration()\n",
    "\n",
    "    #fill holes in the depth information (based on this example: https://nbviewer.jupyter.org/github/IntelRealSense/librealsense/blob/jupyter/notebooks/depth_filters.ipynb)\n",
    "    spatial = rs.spatial_filter()\n",
    "    spatial.set_option(rs.option.filter_magnitude, 2)\n",
    "    spatial.set_option(rs.option.filter_smooth_alpha, 0.5)\n",
    "    spatial.set_option(rs.option.filter_smooth_delta, 20)\n",
    "    spatial.set_option(rs.option.holes_fill, 3)\n",
    "\n",
    "    true_frame_number = []\n",
    "    frame_number = []\n",
    "    time_st = []\n",
    "\n",
    "    num_frame = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames = pipeline.wait_for_frames(100)\n",
    "\n",
    "            this_frame = frames.get_frame_number()  #get frame number\n",
    "\n",
    "            #verify that we have landmarks for this particular frame \n",
    "            landmarks = DF_landmarks.loc[DF_landmarks['BAG_Frame_number'] == this_frame].values\n",
    "\n",
    "            #if there are not landmakrs then just ignore the frame\n",
    "\n",
    "            if len(landmarks)>0 : \n",
    "                #continue only if landmarks for the frame are avaliable\n",
    "                landmarks = landmarks[0][7:]\n",
    "                landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "\n",
    "                if (num_frame != 0) and (true_frame_number[-1] == this_frame): #verify that frame number is not repeated \n",
    "                    #frame is repeated\n",
    "                    aligned_frames = align.process(frames)\n",
    "\n",
    "                    #take color and depth from frame, if any to these is not available then skip the frame\n",
    "                    aligned_depth = aligned_frames.get_depth_frame()\n",
    "                    aligned_color = aligned_frames.get_color_frame()\n",
    "\n",
    "                    # validate that both frames are available\n",
    "                    if not aligned_depth or not aligned_color:\n",
    "                        continue\n",
    "\n",
    "                    time_stamp = frames.get_timestamp()\n",
    "                    true_frame_number[-1] = frames.get_frame_number()\n",
    "                    time_st[-1] = time_stamp\n",
    "                    frame_number[-1] = num_frame\n",
    "\n",
    "                    # Intrinsics & Extrinsics\n",
    "                    depth_intrin = aligned_depth.profile.as_video_stream_profile().intrinsics\n",
    "                    color_intrin = aligned_depth.profile.as_video_stream_profile().intrinsics\n",
    "                    depth_to_color_extrin = aligned_depth.profile.get_extrinsics_to(aligned_color.profile)\n",
    "\n",
    "                    aligned_filtered_depth = spatial.process(aligned_depth)\n",
    "                    depth_frame_array = np.asanyarray(aligned_filtered_depth.get_data())\n",
    "                    depth_frame_array = depth_frame_array*depth_scale\n",
    "\n",
    "                    coords = []\n",
    "                    coords.append(frames.get_frame_number())\n",
    "                    coords.append(int(num_frame)+1)\n",
    "\n",
    "                    for (c,r) in landmarks:  #landmarks provide the x,y position of each landmark. x are columns and y are rows in the figure\n",
    "                        #depth_value = depth_frame.get_distance(int(c),int(r))\n",
    "                        #x,y,z = rs.rs2_deproject_pixel_to_point(depth_intrin, [int(c), int(r)], depth_value)\n",
    "                        depth_value = depth_frame_array[int(r),int(c)]\n",
    "                        z = depth_value\n",
    "                        x = z*((c-depth_intrin.ppx)/depth_intrin.fx)\n",
    "                        y = z*((r-depth_intrin.ppy)/depth_intrin.fy)                    \n",
    "                        coords.append(x),coords.append(y),coords.append(z)\n",
    "\n",
    "                    #DF_3dpositions = DF_3dpositions.append(pd.Series(coords,index = header), ignore_index = True)\n",
    "                    DF_3dpositions.iloc[-1] =  pd.Series(coords,index = header)\n",
    "                else:\n",
    "                    aligned_frames = align.process(frames)\n",
    "\n",
    "                    #take color and depth from frame, if any to these is not available then skip the frame\n",
    "                    aligned_depth = aligned_frames.get_depth_frame()\n",
    "                    aligned_color = aligned_frames.get_color_frame()\n",
    "\n",
    "                    # validate that both frames are available\n",
    "                    if not aligned_depth or not aligned_color:\n",
    "                        continue\n",
    "\n",
    "                    time_stamp = frames.get_timestamp()\n",
    "                    true_frame_number.append(frames.get_frame_number())\n",
    "                    time_st.append(time_stamp)\n",
    "                    frame_number.append(num_frame)\n",
    "\n",
    "                    # Intrinsics & Extrinsics\n",
    "                    depth_intrin = aligned_depth.profile.as_video_stream_profile().intrinsics\n",
    "                    color_intrin = aligned_depth.profile.as_video_stream_profile().intrinsics\n",
    "                    depth_to_color_extrin = aligned_depth.profile.get_extrinsics_to(aligned_color.profile)\n",
    "\n",
    "                    aligned_filtered_depth = spatial.process(aligned_depth)\n",
    "                    depth_frame_array = np.asanyarray(aligned_filtered_depth.as_frame().get_data())\n",
    "                    depth_frame_array = depth_frame_array*depth_scale\n",
    "\n",
    "                    coords = []\n",
    "                    coords.append(frames.get_frame_number())\n",
    "                    coords.append(int(num_frame)+1)\n",
    "\n",
    "                    for (c,r) in landmarks:  #landmarks provide the x,y position of each landmark. x are columns and y are rows in the figure\n",
    "                        #depth_value = depth_frame.get_distance(int(c),int(r))\n",
    "                        #x,y,z = rs.rs2_deproject_pixel_to_point(depth_intrin, [int(c), int(r)], depth_value)\n",
    "                        depth_value = depth_frame_array[int(r),int(c)]\n",
    "                        z = depth_value\n",
    "                        x = z*((c-depth_intrin.ppx)/depth_intrin.fx)\n",
    "                        y = z*((r-depth_intrin.ppy)/depth_intrin.fy)                    \n",
    "                        coords.append(x),coords.append(y),coords.append(z)\n",
    "\n",
    "                    DF_3dpositions = DF_3dpositions.append(pd.Series(coords,index = header), ignore_index = True)\n",
    "\n",
    "                    num_frame += 1\n",
    "\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "\n",
    "\n",
    "    #add time to 3d coordinates\n",
    "    DF_3dpositions.insert(loc=1, column='Time_Stamp (s)', value=(np.array(time_st)-time_st[0])/1000)\n",
    "    landmarks_3D_file = BAG_File[:-4]+'_Landmarks3D.csv'\n",
    "    DF_3dpositions.to_csv(landmarks_3D_file)\n",
    "\n",
    "    cvs_frame_info = BAG_File[:-4]+'_frameInfoDepth.csv'\n",
    "    DF = pd.DataFrame()\n",
    "    DF['Actual_Frame_Number'] = true_frame_number\n",
    "    DF['Frame_Time_Stamp'] = (np.array(time_st)-time_st[0])/1000\n",
    "    DF['Frame_Number_in_Video'] = frame_number\n",
    "    print(cvs_frame_info)\n",
    "    DF.to_csv(cvs_frame_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/45 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.025989434201982\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                 | 1/45 [00:56<41:44, 56.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "26.341420461332316\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_SLOW_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_SLOW_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_SLOW_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_BBP_SLOW_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▋                                                                               | 2/45 [01:57<41:41, 58.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.9504141400607\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_FAST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_FAST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_FAST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_FAST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 3/45 [02:50<39:26, 56.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.407120106083273\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_DDK_PA_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▍                                                                           | 4/45 [03:59<41:11, 60.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.960578199453472\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▏                                                                         | 5/45 [05:09<42:11, 63.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.277217439719042\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_BIGSMILE_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████                                                                        | 6/45 [05:47<36:11, 55.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.644586433284534\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▉                                                                      | 7/45 [06:36<33:57, 53.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.897042744860542\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_SLOW_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_SLOW_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_SLOW_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OOEE_SLOW_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▊                                                                    | 8/45 [07:54<37:29, 60.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.12516504840839\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_DIS_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_DIS_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_DIS_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_DIS_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 9/45 [08:38<33:35, 55.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.11866227485478\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_FAST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_FAST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_FAST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_FAST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▏                                                               | 10/45 [09:01<26:53, 46.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.966792273990304\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████                                                              | 11/45 [10:25<32:26, 57.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.343857796354527\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_OPEN_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▊                                                            | 12/45 [11:07<28:58, 52.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.285448081346\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████▋                                                          | 13/45 [12:11<29:59, 56.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.412637647801848\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_NSM_SPREAD_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▌                                                        | 14/45 [12:46<25:43, 49.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.55084890673524\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_RST_REST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_RST_REST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_RST_REST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\DJ01_02_RST_REST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▎                                                      | 15/45 [13:26<23:24, 46.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "26.293003289533107\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▏                                                    | 16/45 [14:10<22:16, 46.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "25.99228773361202\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_SLOW_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_SLOW_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_SLOW_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_BBP_SLOW_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████▉                                                   | 17/45 [15:20<24:52, 53.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.276045783610602\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_FAST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_FAST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_FAST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_FAST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 18/45 [16:14<23:59, 53.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.874520267774372\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_DDK_PA_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▌                                               | 19/45 [17:03<22:30, 51.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.62176338914537\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████▍                                             | 20/45 [19:06<30:31, 73.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.762193203272496\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_BIGSMILE_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▎                                           | 21/45 [20:06<27:47, 69.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.35043398322507\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████████                                          | 22/45 [20:58<24:37, 64.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.491885159943507\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_SLOW_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_SLOW_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_SLOW_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OOEE_SLOW_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████▉                                        | 23/45 [23:00<29:52, 81.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.945991186300994\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_DIS_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_DIS_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_DIS_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_DIS_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████▋                                      | 24/45 [24:03<26:35, 75.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.423857368460773\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_FAST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_FAST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_FAST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_FAST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▌                                    | 25/45 [24:39<21:21, 64.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.7690167087089\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████▍                                  | 26/45 [26:52<26:48, 84.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.843971102316186\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_OPEN_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 27/45 [27:40<22:04, 73.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.104682401665563\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████                               | 28/45 [29:40<24:48, 87.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.85955874723339\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_NORM_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_NORM_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_NORM_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_NSM_SPREAD_NORM_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▊                             | 29/45 [30:44<21:27, 80.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.49082419072393\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_RST_REST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_RST_REST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_RST_REST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\MK01_02_RST_REST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 30/45 [31:24<17:06, 68.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "23.386744424236163\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████▍                         | 31/45 [32:13<14:33, 62.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "24.491365333106174\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_SLOW_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_SLOW_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_SLOW_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_BBP_SLOW_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████████████████████████████████▎                       | 32/45 [33:50<15:48, 72.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.824851789754913\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_FAST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_FAST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_FAST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_FAST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████▏                     | 33/45 [35:01<14:28, 72.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.77772424612593\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_DDK_PA_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████████████▉                    | 34/45 [36:09<13:00, 70.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.47899796690162\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████▊                  | 35/45 [37:46<13:09, 78.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.00554973903177\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_BIGSMILE_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 36/45 [38:30<10:14, 68.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.23806867335626\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████▍              | 37/45 [39:13<08:07, 60.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "26.531271115950897\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_SLOW_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_SLOW_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_SLOW_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OOEE_SLOW_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████▏            | 38/45 [41:15<09:12, 78.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.459922242542884\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_DIS_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_DIS_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_DIS_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_DIS_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████           | 39/45 [42:15<07:20, 73.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.00661115291014\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_FAST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_FAST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_FAST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_FAST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 40/45 [42:48<05:06, 61.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "27.071877573518638\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████████████████████████████████████████▋       | 41/45 [44:32<04:56, 74.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.73140672052895\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_OPEN_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████▌     | 42/45 [45:31<03:28, 69.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "28.290168751314372\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_HOLD_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_HOLD_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_HOLD_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_HOLD_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████▎   | 43/45 [47:08<02:35, 77.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.29643154815528\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_NORMAL_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_NORMAL_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_NORMAL_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_NSM_SPREAD_NORMAL_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████▏ | 44/45 [47:53<01:08, 68.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n",
      "29.463569305579995\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_RST_REST_color.mp4\n",
      "here4\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_RST_REST_frameInfoColor.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_RST_REST_landmarks.csv\n",
      "C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data\\RM4_02_RST_REST_frameInfoDepth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [48:32<00:00, 59.28s/it]"
     ]
    }
   ],
   "source": [
    "# get all files in path and process each one. Show progress with a bar\n",
    "paths = [r'C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\deniz_data']\n",
    "#r'C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\example'\n",
    "\n",
    "for path in paths:\n",
    "    Files = os.listdir(path)            \n",
    "    ext=('.bag')\n",
    "    Files = [i for i in Files if i.endswith(tuple(ext))]\n",
    "    Files_select = []\n",
    "    for k,f in enumerate(Files):\n",
    "        Files[k] = os.path.join(path,f)\n",
    "        #if ('BIGSMILE' in f): \n",
    "        Files_select.append(Files[k])\n",
    "           \n",
    "    pbar = tqdm(total=len(Files_select))\n",
    "\n",
    "    for f in Files_select:\n",
    "        try: \n",
    "            get_color_from_bag(f)\n",
    "            get_landmarks(f, face_detection_net, face_alignment_net)\n",
    "            smooth_landmarks(f)\n",
    "            get_3d_data(f)\n",
    "        except:\n",
    "            print('the file in exception is:')\n",
    "            print(f)\n",
    "            print()\n",
    "\n",
    "        pbar.update(1)\n",
    "    \n",
    "# def do_work(f, models):\n",
    "#     face_detection_net = models['FaceDetection']\n",
    "#     face_alignment_net = models['FaceAlignment']\n",
    "    \n",
    "#     get_color_from_bag(f)\n",
    "#     get_landmarks(f, face_detection_net, face_alignment_net)\n",
    "#     smooth_landmarks(f)\n",
    "#     get_3d_data(f)\n",
    "#     pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_BBP_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_BBP_SLOW.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_DDK_PA_FAST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_DDK_PA_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_BIGSMILE_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_BIGSMILE_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_OOEE_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_OOEE_SLOW.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_OPEN_DIS.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_OPEN_FAST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_OPEN_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_OPEN_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_SPREAD_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_NSM_SPREAD_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\DJ01_02_RST_REST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_BBP_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_BBP_SLOW.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_DDK_PA_FAST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_DDK_PA_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_BIGSMILE_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_BIGSMILE_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_OOEE_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_OOEE_SLOW.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_OPEN_DIS.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_OPEN_FAST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_OPEN_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_OPEN_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_SPREAD_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_NSM_SPREAD_NORM.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\MK01_02_RST_REST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_BBP_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_BBP_SLOW.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_DDK_PA_FAST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_DDK_PA_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_BIGSMILE_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_BIGSMILE_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_OOEE_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_OOEE_SLOW.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_OPEN_DIS.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_OPEN_FAST.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_OPEN_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_OPEN_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_SPREAD_HOLD.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_NSM_SPREAD_NORMAL.bag',\n",
       " 'C:\\\\Users\\\\GuarinD\\\\Documents\\\\GitHub\\\\Face_and_Gestures_2020\\\\deniz_data\\\\RM4_02_RST_REST.bag']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Files_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth landmarks positions and generate a new video showing landmarks\n",
    "if 0:\n",
    "    BAG_File = r\"C:\\Users\\GuarinD\\Documents\\GitHub\\Face_and_Gestures_2020\\example\\N_02_NSM_SPREAD.bag\"\n",
    "    LandmarkDataFrame = pd.read_csv(BAG_File[:-4]+'_landmarks.csv', index_col=0)\n",
    "    windowlength=5\n",
    "    for i in range(68):\n",
    "        num=str(i)\n",
    "        xx = LandmarkDataFrame['landmark_'+num+'_x'].values\n",
    "        xx_med = signal.medfilt(xx,kernel_size=windowlength)\n",
    "\n",
    "        yy = LandmarkDataFrame['landmark_'+num+'_y'].values\n",
    "        yy_med = signal.medfilt(yy,kernel_size=windowlength)  \n",
    "\n",
    "        LandmarkDataFrame['landmark_'+num+'_x'] = xx_med\n",
    "        LandmarkDataFrame['landmark_'+num+'_y'] = yy_med\n",
    "\n",
    "    landmark_file = BAG_File[:-4]+'_landmarksFiltered.csv'\n",
    "    LandmarkDataFrame.to_csv(landmark_file)    \n",
    "\n",
    "    color_file = BAG_File[:-4]+'_color.mp4'\n",
    "    video_handler = cv2.VideoCapture(color_file)  # read the video\n",
    "    FPS = video_handler.get(cv2.CAP_PROP_FPS)\n",
    "    width = video_handler.get(3)   # float\n",
    "    height = video_handler.get(4) # float\n",
    "\n",
    "    color_file_landmark = BAG_File[:-4]+'_colorlandmark.mp4'\n",
    "    video = VideoWriter(color_file_landmark, -1, int(FPS), (int(width),int(height)))\n",
    "\n",
    "    success = True \n",
    "    k=0\n",
    "    for k in range(len(LandmarkDataFrame)):\n",
    "        success, image = video_handler.read()\n",
    "\n",
    "        frame_number=k+1\n",
    "        frame_information = LandmarkDataFrame.loc[LandmarkDataFrame['Video_Frame_number'] == frame_number].values\n",
    "        shape = np.array([frame_information[0][7:]])\n",
    "        shape = np.reshape(shape.astype(np.int), (-1, 2))\n",
    "        bb = np.array([frame_information[0][3:7]])\n",
    "\n",
    "\n",
    "        for j, (x, y) in enumerate(shape):\n",
    "            if x is np.NaN:\n",
    "                continue\n",
    "            elif j<17:\n",
    "                continue\n",
    "            else:\n",
    "                cv2.rectangle(image, (int(bb[0][0]), int(bb[0][1])), (int(bb[0][2]), int(bb[0][3])), (255,0,0), 1)\n",
    "                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "\n",
    "        frame_to_save = image\n",
    "        video.write(frame_to_save)\n",
    "        k +=1\n",
    "\n",
    "    video.release()\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColorFromBag_newFolder(BAG_File, new_path=None):\n",
    "\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "\n",
    "    rs.config.enable_device_from_file(config, BAG_File, repeat_playback=False)\n",
    "\n",
    "    config.enable_all_streams()\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # create alignment object\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    # inform the device that this is not live streaming from camera\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(False)\n",
    "    duration = playback.get_duration()\n",
    "\n",
    "    true_frame_number = []\n",
    "    frame_number = []\n",
    "    time_st = []\n",
    "\n",
    "    num_frame = 0\n",
    "\n",
    "\n",
    "    Color_Frames = []#{}\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames = pipeline.wait_for_frames()  #get frame from file \n",
    "\n",
    "            this_frame = frames.get_frame_number()  #get frame number \n",
    "\n",
    "            if (num_frame != 0) and (true_frame_number[-1] == this_frame): #verify that frame number is not repeated \n",
    "                #if frame number is repeated then replace the stored information \n",
    "                aligned_frames = align.process(frames)\n",
    "\n",
    "                #take color and depth from frame, if any to these is not available then skip the frame\n",
    "                aligned_depth = aligned_frames.get_depth_frame()\n",
    "                aligned_color = aligned_frames.get_color_frame()\n",
    "\n",
    "                # validate that both frames are available\n",
    "                if not aligned_depth or not aligned_color:\n",
    "                    continue\n",
    "\n",
    "                time_stamp = frames.get_timestamp()\n",
    "                true_frame_number[-1] = frames.get_frame_number()\n",
    "                time_st[-1] = time_stamp \n",
    "\n",
    "                # transform to np array\n",
    "\n",
    "                color_data = np.asanyarray(aligned_color.as_frame().get_data(), dtype=np.int)\n",
    "                #depth_data = np.asanyarray(aligned_depth.as_frame().get_data(), dtype=np.int)\n",
    "                # adjust depth data in meters\n",
    "                #depth_data *= depth_scale\n",
    "\n",
    "                Color_Frames[-1] = color_data\n",
    "\n",
    "            else:\n",
    "                #if frame number is not repeated then append the stored information \n",
    "                aligned_frames = align.process(frames)\n",
    "\n",
    "                #take color and depth from frame, if any to these is not available then skip the frame\n",
    "                aligned_depth = aligned_frames.get_depth_frame()\n",
    "                aligned_color = aligned_frames.get_color_frame()\n",
    "\n",
    "                # validate that both frames are available\n",
    "                if not aligned_depth or not aligned_color:\n",
    "                    continue\n",
    "\n",
    "                time_stamp = frames.get_timestamp()\n",
    "                true_frame_number.append(frames.get_frame_number())\n",
    "                time_st.append(time_stamp )\n",
    "\n",
    "                # transform to np array\n",
    "\n",
    "                color_data = np.asanyarray(aligned_color.as_frame().get_data(), dtype=np.int)\n",
    "                #depth_data = np.asanyarray(aligned_depth.as_frame().get_data(), dtype=np.int)\n",
    "                # adjust depth data in meters\n",
    "                #depth_data *= depth_scale\n",
    "\n",
    "                Color_Frames.append(color_data)\n",
    "                #Depth_Frames.append(depth_data\n",
    "\n",
    "                frame_number.append(num_frame)\n",
    "                num_frame += 1\n",
    "\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    finally:\n",
    "        pipeline.stop()\n",
    " \n",
    "\n",
    "    try: \n",
    "        duration_movie = duration.total_seconds()\n",
    "        FPS = num_frame/duration_movie\n",
    "        height, width,_ =  Color_Frames[0].shape\n",
    "\n",
    "\n",
    "        root, file_name = os.path.split(BAG_File)\n",
    "        \n",
    "        if new_path is not None:\n",
    "            root = new_path\n",
    "\n",
    "        print(root)\n",
    "        color_file = os.path.join(root,file_name[:-4]+'_color.mp4')\n",
    "\n",
    "        video = VideoWriter(color_file, 0, int(FPS), (width,height))\n",
    "\n",
    "        for k in range(num_frame):\n",
    "            frame_to_save = Color_Frames[k].astype('uint8')\n",
    "            video.write(frame_to_save)\n",
    "\n",
    "        video.release()    \n",
    "\n",
    "\n",
    "        cvs_frame_info = os.path.join(root,file_name[:-4]+'_frameInfoColor.csv')\n",
    "        df_cols = ['Actual_Frame_Number', 'Frame_Time_Stamp', 'Frame_Number_in_Video']\n",
    "        df = pd.DataFrame(columns=df_cols)\n",
    "        df['Actual_Frame_Number'] = true_frame_number\n",
    "        df['Frame_Time_Stamp'] = (np.array(time_st)-time_st[0])/1000\n",
    "        df['Frame_Number_in_Video'] = frame_number\n",
    "\n",
    "        df.to_csv(cvs_frame_info)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    Files_dataFrame = pd.read_csv(r\"C:\\Users\\GuarinD\\Documents\\color_videos\\files.csv\")\n",
    "    paths_to_save = [r'C:\\Users\\GuarinD\\Documents\\color_videos\\PD', r'C:\\Users\\GuarinD\\Documents\\color_videos\\NF']\n",
    "\n",
    "    Files = Files_dataFrame['File_Name']\n",
    "\n",
    "    getColorFromBag_newFolder(Files[7], new_path=paths_to_save[1])\n",
    "\n",
    "\n",
    "    for k, file in enumerate(Files):\n",
    "        if k < 24:\n",
    "            continue \n",
    "\n",
    "        subject_ID = Files_dataFrame['Subject_ID'][k]\n",
    "        if 'PD' in subject_ID:\n",
    "            getColorFromBag_newFolder(file, new_path=paths_to_save[0])\n",
    "        else:\n",
    "            getColorFromBag_newFolder(file, new_path=paths_to_save[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer files from one location to another based on name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
